{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EsmForMaskedLM, EsmTokenizer\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "model_name = \"facebook/esm2_t6_8M_UR50D\"  # Example ESM model\n",
    "model = EsmForMaskedLM.from_pretrained(model_name)\n",
    "tokenizer = EsmTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Protein sequence with a masked token\n",
    "sequence = \"MKTLLILAVVFCALMAIVFV<mask>\"\n",
    "\n",
    "# Tokenize the input sequence\n",
    "inputs = tokenizer(sequence, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "# Extract input IDs and attention mask\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Logits for masked language modeling\n",
    "logits = outputs.logits\n",
    "print(logits.shape)  # Shape: (batch_size, seq_len, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Find the index of the masked token\n",
    "masked_index = (input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "\n",
    "# Get logits for the masked token\n",
    "masked_logits = logits[masked_index]\n",
    "\n",
    "# Predicted token ID\n",
    "predicted_token_id = torch.argmax(masked_logits, dim=-1)\n",
    "\n",
    "# Convert token ID back to the residue\n",
    "predicted_token = tokenizer.convert_ids_to_tokens(predicted_token_id)\n",
    "print(f\"Predicted token: {predicted_token}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Mask some tokens in the sequence\n",
    "labels = input_ids.clone()\n",
    "labels[labels != tokenizer.mask_token_id] = -100  # Ignore non-masked tokens for loss\n",
    "\n",
    "# Forward pass with labels\n",
    "outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "loss = outputs.loss\n",
    "print(f\"Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token: ['V', 'L', 'L', 'L', 'R']\n"
     ]
    }
   ],
   "source": [
    "from transformers import EsmForMaskedLM, EsmTokenizer\n",
    "import torch\n",
    "\n",
    "# Load pretrained ESM model and tokenizer\n",
    "model_name = \"facebook/esm2_t6_8M_UR50D\"\n",
    "model = EsmForMaskedLM.from_pretrained(model_name)\n",
    "tokenizer = EsmTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Protein sequence\n",
    "sequence = \"MKTLLILAVVFCALMAIVFV<mask><mask><mask><mask><mask>\"\n",
    "\n",
    "# Tokenize the sequence\n",
    "inputs = tokenizer(sequence, return_tensors=\"pt\", add_special_tokens=True)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Predict masked token\n",
    "masked_index = (input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "masked_logits = logits[masked_index]\n",
    "predicted_token_id = torch.argmax(masked_logits, dim=-1)\n",
    "predicted_token = tokenizer.convert_ids_to_tokens(predicted_token_id)\n",
    "\n",
    "print(f\"Predicted token: {predicted_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/guillaumebelissent/opt/anaconda3/envs/ml/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/guillaumebelissent/opt/anaconda3/envs/ml/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/guillaumebelissent/opt/anaconda3/envs/ml/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/guillaumebelissent/opt/anaconda3/envs/ml/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/guillaumebelissent/opt/anaconda3/envs/ml/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/guillaumebelissent/opt/anaconda3/envs/ml/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/guillaumebelissent/opt/anaconda3/envs/ml/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/guillaumebelissent/opt/anaconda3/envs/ml/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/guillaumebelissent/opt/anaconda3/envs/ml/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/guillaumebelissent/opt/anaconda3/envs/ml/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/guillaumebelissent/opt/anaconda3/envs/ml/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/guillaumebelissent/opt/anaconda3/envs/ml/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/guillaumebelissent/opt/anaconda3/envs/ml/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/guillaumebelissent/opt/anaconda3/envs/ml/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/guillaumebelissent/opt/anaconda3/envs/ml/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/guillaumebelissent/opt/anaconda3/envs/ml/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/guillaumebelissent/opt/anaconda3/envs/ml/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/guillaumebelissent/opt/anaconda3/envs/ml/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/guillaumebelissent/opt/anaconda3/envs/ml/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/guillaumebelissent/opt/anaconda3/envs/ml/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/6_/6g10b43x17ngwmk4l9msb2mc0000gn/T/ipykernel_40076/2442966630.py\", line 1, in <module>\n",
      "    from esm import pretrained\n",
      "  File \"/Users/guillaumebelissent/opt/anaconda3/envs/ml/lib/python3.11/site-packages/esm/__init__.py\", line 8, in <module>\n",
      "    from .data import Alphabet, BatchConverter, FastaBatchedDataset  # noqa\n",
      "  File \"/Users/guillaumebelissent/opt/anaconda3/envs/ml/lib/python3.11/site-packages/esm/data.py\", line 12, in <module>\n",
      "    import torch\n",
      "  File \"/Users/guillaumebelissent/opt/anaconda3/envs/ml/lib/python3.11/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/guillaumebelissent/opt/anaconda3/envs/ml/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/guillaumebelissent/opt/anaconda3/envs/ml/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/guillaumebelissent/opt/anaconda3/envs/ml/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/guillaumebelissent/opt/anaconda3/envs/ml/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/guillaumebelissent/opt/anaconda3/envs/ml/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 21])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch_tokens\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# (num_sequences, msa_depth, seq_length)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     22\u001b[0m     results \u001b[38;5;241m=\u001b[39m model(batch_tokens, repr_layers\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m12\u001b[39m])\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Extract token embeddings from the last layer (layer 12)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from esm import pretrained\n",
    "\n",
    "# Load the ESM-MSA model\n",
    "model, alphabet = pretrained.esm_msa1b_t12_100M_UR50S()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "# Example MSA\n",
    "msa = [\n",
    "    (\"seq1\", \"MKTLLILAVVFCALMAIVFV\"),\n",
    "    (\"seq2\", \"MKTLIILVVFCALMAVVVF.\"),\n",
    "    (\"seq3\", \"MKTLLILVVFCALMAIVF..\"),\n",
    "]\n",
    "\n",
    "# Convert MSA into model-compatible format\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter([msa])\n",
    "\n",
    "# Print batch tokens shape\n",
    "print(batch_tokens.shape)  # (num_sequences, msa_depth, seq_length)\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    results = model(batch_tokens, repr_layers=[12])\n",
    "\n",
    "# Extract token embeddings from the last layer (layer 12)\n",
    "msa_embeddings = results[\"representations\"][12]\n",
    "print(msa_embeddings.shape)  # Shape: (msa_depth, seq_length, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t6_8M_UR50D.pt\" to /Users/guillaumebelissent/.cache/torch/hub/checkpoints/esm2_t6_8M_UR50D.pt\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t6_8M_UR50D-contact-regression.pt\" to /Users/guillaumebelissent/.cache/torch/hub/checkpoints/esm2_t6_8M_UR50D-contact-regression.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token at position 10: C\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import esm\n",
    "\n",
    "# Load the MSA model and alphabet\n",
    "model, alphabet = esm.pretrained.esm_msa1b_t12_100M_UR50S()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Example MSA (aligned sequences)\n",
    "msa = [\n",
    "    (\"seq1\", \"MKTLLILAVVFCALMAIVFV\"),\n",
    "    (\"seq2\", \"MKTLIILVVFCALMAVVVF.\"),\n",
    "    (\"seq3\", \"MKTLLILVVFCALMAIVF..\"),\n",
    "]\n",
    "\n",
    "# Tokenize the MSA\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter([msa])\n",
    "\n",
    "# Move tokens to the appropriate device\n",
    "batch_tokens = batch_tokens.to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    results = model(batch_tokens)\n",
    "\n",
    "# Extract per-residue embeddings\n",
    "# Layer 12 corresponds to the final layer of the model\n",
    "representations = results[\"representations\"][12]  # Shape: (msa_depth, seq_len, hidden_size)\n",
    "print(\"Per-residue embedding shape:\", representations.shape)\n",
    "\n",
    "query_sequence_embedding = representations[0]  # Shape: (seq_len, hidden_size)\n",
    "\n",
    "# Example MSA with a masked token in the first sequence\n",
    "msa_with_mask = [\n",
    "    (\"seq1\", \"MKTLLILAV<mask>CALMAIVFV\"),  # Masked token at position 10\n",
    "    (\"seq2\", \"MKTLIILVVFCALMAVVVF.\"),\n",
    "    (\"seq3\", \"MKTLLILVVFCALMAIVF..\"),\n",
    "]\n",
    "\n",
    "# Tokenize the MSA with masking\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter([msa_with_mask])\n",
    "batch_tokens = batch_tokens.to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    results = model(batch_tokens)\n",
    "\n",
    "# Extract logits for the masked position\n",
    "masked_index = 10 + 1  # Adjust for the [CLS] token\n",
    "masked_logits = results[\"logits\"][0, 0, masked_index]  # First sequence, masked position\n",
    "predicted_token_id = masked_logits.argmax().item()\n",
    "predicted_token = alphabet.get_tok(predicted_token_id)\n",
    "print(f\"Predicted token at position 10: {predicted_token}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token at position 54: A\n"
     ]
    }
   ],
   "source": [
    "# Load the model and alphabet\n",
    "model, alphabet = esm.pretrained.esm_msa1b_t12_100M_UR50S()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Example MSA with a masked token\n",
    "msa_with_mask = [\n",
    "    (\"P69905\",\"MVLSPADKTNVKAAWGKVGAHAGEYGAEALERMFLSFPTTKTYFPHFDLSHGS<mask>QVKGHGKKVADALTNAVAHVDDMPNALSALSDLHAHKLRVDPVNFKLLSHCLLVTLAAHLPAEFTPAVHASLDKFLASVSTVLTSKYR\"),\n",
    "    (\"P01942\",\"MVLSGEDKSNIKAAWGKIGGHGAEYGAEALERMFASFPTTKTYFPHFDVSHGS<mask>QVKGHGKKVADALASAAGHLDDLPGALSALSDLHAHKLRVDPVNFKLLSHCLLVTLASHHPADFTPAVHASLDKFLASVSTVLTSKYR\"),\n",
    "    (\"P13786\",\"MSLTRTERTIILSLWSKISTQADVIGTETLERLFSCYPQAKTYFPHFDLHSGS<mask>QLRAHGSKVVAAVGDAVKSIDNVTSALSKLSELHAYVLRVDPVNFKFLSHCLLVTLASHFPADFTADAHAAWDKFLSIVSGVLTEKYR\"),\n",
    "]\n",
    "\n",
    "# Tokenize the MSA\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(msa_with_mask)\n",
    "batch_tokens = batch_tokens.to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    results = model(batch_tokens)\n",
    "\n",
    "# Extract logits for the masked position\n",
    "masked_index = 1+msa_with_mask[0][1].find('<')  # Adjust for the [CLS] token\n",
    "masked_logits = results[\"logits\"][0, 0, masked_index]  # First sequence, masked position\n",
    "predicted_token_id = masked_logits.argmax().item()\n",
    "predicted_token = alphabet.get_tok(predicted_token_id)\n",
    "print(f\"Predicted token at position {masked_index}: {predicted_token}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0, 20, 15, 11,  4,  4, 12,  4,  5,  7,  7, 18, 23,  5,  4, 20,  5,\n",
       "          12,  7, 18,  7],\n",
       "         [ 0, 20, 15, 11,  4, 12, 12,  4,  7,  7, 18, 23,  5,  4, 20,  5,  7,\n",
       "           7,  7, 18, 29],\n",
       "         [ 0, 20, 15, 11,  4,  4, 12,  4,  7,  7, 18, 23,  5,  4, 20,  5, 12,\n",
       "           7, 18, 29, 29]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa = [\n",
    "    \"MVLSPADKTNVKAAWGKVGAHAGEYGAEALERMFLSFPTTKTYFPHFDLSHGSAQVKGHGKKVADALTNAVAHVDDMPNALSALSDLHAHKLRVDPVNFKLLSHCLLVTLAAHLPAEFTPAVHASLDKFLASVSTVLTSKYR\",\n",
    "    \"MVLSGEDKSNIKAAWGKIGGHGAEYGAEALERMFASFPTTKTYFPHFDVSHGSAQVKGHGKKVADALASAAGHLDDLPGALSALSDLHAHKLRVDPVNFKLLSHCLLVTLASHHPADFTPAVHASLDKFLASVSTVLTSKYR\",\n",
    "    \"MSLTRTERTIILSLWSKISTQADVIGTETLERLFSCYPQAKTYFPHFDLHSGSAQLRAHGSKVVAAVGDAVKSIDNVTSALSKLSELHAYVLRVDPVNFKFLSHCLLVTLASHFPADFTADAHAAWDKFLSIVSGVLTEKYR\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "esm2_t30_150M_UR50D=pretrained.esm2_t30_150M_UR50D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(sequence, position):\n",
    "    return sequence[: position] + '<mask>' + sequence[position + 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "blosum62 = {\n",
    "        ('A', 'A'):  4, ('A', 'R'): -1, ('A', 'N'): -2, ('A', 'D'): -2, ('A', 'C'):  0,\n",
    "        ('A', 'Q'): -1, ('A', 'E'): -1, ('A', 'G'):  0, ('A', 'H'): -2, ('A', 'I'): -1,\n",
    "        ('A', 'L'): -1, ('A', 'K'): -1, ('A', 'M'): -1, ('A', 'F'): -2, ('A', 'P'): -1,\n",
    "        ('A', 'S'):  1, ('A', 'T'):  0, ('A', 'W'): -3, ('A', 'Y'): -2, ('A', 'V'):  0,\n",
    "        ('R', 'R'):  5, ('R', 'N'):  0, ('R', 'D'): -2, ('R', 'C'): -3, ('R', 'Q'):  1,\n",
    "        ('R', 'E'):  0, ('R', 'G'): -2, ('R', 'H'):  0, ('R', 'I'): -3, ('R', 'L'): -2,\n",
    "        ('R', 'K'):  2, ('R', 'M'): -1, ('R', 'F'): -3, ('R', 'P'): -2, ('R', 'S'): -1,\n",
    "        ('R', 'T'): -1, ('R', 'W'): -3, ('R', 'Y'): -2, ('R', 'V'): -3,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(blosum62.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blosum62(AA1, AA2):\n",
    "    blosum62 = {\n",
    "        ('A', 'A'):  4, ('A', 'R'): -1, ('A', 'N'): -2, ('A', 'D'): -2, ('A', 'C'):  0,\n",
    "        ('A', 'Q'): -1, ('A', 'E'): -1, ('A', 'G'):  0, ('A', 'H'): -2, ('A', 'I'): -1,\n",
    "        ('A', 'L'): -1, ('A', 'K'): -1, ('A', 'M'): -1, ('A', 'F'): -2, ('A', 'P'): -1,\n",
    "        ('A', 'S'):  1, ('A', 'T'):  0, ('A', 'W'): -3, ('A', 'Y'): -2, ('A', 'V'):  0,\n",
    "        ('R', 'R'):  5, ('R', 'N'):  0, ('R', 'D'): -2, ('R', 'C'): -3, ('R', 'Q'):  1,\n",
    "        ('R', 'E'):  0, ('R', 'G'): -2, ('R', 'H'):  0, ('R', 'I'): -3, ('R', 'L'): -2,\n",
    "        ('R', 'K'):  2, ('R', 'M'): -1, ('R', 'F'): -3, ('R', 'P'): -2, ('R', 'S'): -1,\n",
    "        ('R', 'T'): -1, ('R', 'W'): -3, ('R', 'Y'): -2, ('R', 'V'): -3,\n",
    "        ('N', 'N'):  6, ('N', 'D'):  1, ('N', 'C'): -3, ('N', 'Q'):  0, ('N', 'E'):  0,\n",
    "        ('N', 'G'):  0, ('N', 'H'):  1, ('N', 'I'): -3, ('N', 'L'): -3, ('N', 'K'):  0,\n",
    "        ('N', 'M'): -2, ('N', 'F'): -3, ('N', 'P'): -2, ('N', 'S'):  1, ('N', 'T'):  0,\n",
    "        ('N', 'W'): -4, ('N', 'Y'): -2, ('N', 'V'): -3,\n",
    "        ('D', 'D'):  6, ('D', 'C'): -3, ('D', 'Q'):  0, ('D', 'E'):  2, ('D', 'G'): -1,\n",
    "        ('D', 'H'): -1, ('D', 'I'): -3, ('D', 'L'): -4, ('D', 'K'): -1, ('D', 'M'): -3,\n",
    "        ('D', 'F'): -3, ('D', 'P'): -1, ('D', 'S'):  0, ('D', 'T'): -1, ('D', 'W'): -4,\n",
    "        ('D', 'Y'): -3, ('D', 'V'): -3,\n",
    "        ('C', 'C'):  9, ('C', 'Q'): -3, ('C', 'E'): -4, ('C', 'G'): -3, ('C', 'H'): -3,\n",
    "        ('C', 'I'): -1, ('C', 'L'): -1, ('C', 'K'): -3, ('C', 'M'): -1, ('C', 'F'): -2,\n",
    "        ('C', 'P'): -3, ('C', 'S'): -1, ('C', 'T'): -1, ('C', 'W'): -2, ('C', 'Y'): -2,\n",
    "        ('C', 'V'): -1,\n",
    "        ('Q', 'Q'):  5, ('Q', 'E'):  2, ('Q', 'G'): -2, ('Q', 'H'):  0, ('Q', 'I'): -3,\n",
    "        ('Q', 'L'): -2, ('Q', 'K'):  1, ('Q', 'M'):  0, ('Q', 'F'): -3, ('Q', 'P'): -1,\n",
    "        ('Q', 'S'):  0, ('Q', 'T'): -1, ('Q', 'W'): -2, ('Q', 'Y'): -1, ('Q', 'V'): -2,\n",
    "        ('E', 'E'):  5, ('E', 'G'): -2, ('E', 'H'):  0, ('E', 'I'): -3, ('E', 'L'): -3,\n",
    "        ('E', 'K'):  1, ('E', 'M'): -2, ('E', 'F'): -3, ('E', 'P'): -1, ('E', 'S'):  0,\n",
    "        ('E', 'T'): -1, ('E', 'W'): -3, ('E', 'Y'): -2, ('E', 'V'): -2,\n",
    "        ('G', 'G'):  6, ('G', 'H'): -2, ('G', 'I'): -4, ('G', 'L'): -4, ('G', 'K'): -2,\n",
    "        ('G', 'M'): -3, ('G', 'F'): -3, ('G', 'P'): -2, ('G', 'S'):  0, ('G', 'T'): -2,\n",
    "        ('G', 'W'): -2, ('G', 'Y'): -3, ('G', 'V'): -3,\n",
    "        ('H', 'H'):  8, ('H', 'I'): -3, ('H', 'L'): -3, ('H', 'K'): -1, ('H', 'M'): -2,\n",
    "        ('H', 'F'): -1, ('H', 'P'): -2, ('H', 'S'): -1, ('H', 'T'): -2, ('H', 'W'): -2,\n",
    "        ('H', 'Y'):  2, ('H', 'V'): -3,\n",
    "        ('I', 'I'):  4, ('I', 'L'):  2, ('I', 'K'): -3, ('I', 'M'):  1, ('I', 'F'):  0,\n",
    "        ('I', 'P'): -3, ('I', 'S'): -2, ('I', 'T'): -1, ('I', 'W'): -3, ('I', 'Y'): -1,\n",
    "        ('I', 'V'):  3,\n",
    "        ('L', 'L'):  4, ('L', 'K'): -2, ('L', 'M'):  2, ('L', 'F'):  0, ('L', 'P'): -3,\n",
    "        ('L', 'S'): -2, ('L', 'T'): -1, ('L', 'W'): -2, ('L', 'Y'): -1, ('L', 'V'):  1,\n",
    "        ('K', 'K'):  5, ('K', 'M'): -1, ('K', 'F'): -3, ('K', 'P'): -1, ('K', 'S'):  0,\n",
    "        ('K', 'T'): -1, ('K', 'W'): -3, ('K', 'Y'): -2, ('K', 'V'): -2,\n",
    "        ('M', 'M'):  5, ('M', 'F'):  0, ('M', 'P'): -2, ('M', 'S'): -1, ('M', 'T'): -1,\n",
    "        ('M', 'W'): -1, ('M', 'Y'): -1, ('M', 'V'):  1,\n",
    "        ('F', 'F'):  6, ('F', 'P'): -4, ('F', 'S'): -2, ('F', 'T'): -2, ('F', 'W'):  1,\n",
    "        ('F', 'Y'):  3, ('F', 'V'): -1,\n",
    "        ('P', 'P'):  7, ('P', 'S'): -1, ('P', 'T'): -1, ('P', 'W'): -4, ('P', 'Y'): -3,\n",
    "        ('P', 'V'): -2,\n",
    "        ('S', 'S'):  4, ('S', 'T'):  1, ('S', 'W'): -3, ('S', 'Y'): -2, ('S', 'V'): -2,\n",
    "        ('T', 'T'):  5, ('T', 'W'): -2, ('T', 'Y'): -2, ('T', 'V'):  0,\n",
    "        ('W', 'W'): 11, ('W', 'Y'):  2, ('W', 'V'): -3,\n",
    "        ('Y', 'Y'):  7, ('Y', 'V'): -1,\n",
    "        ('V', 'V'):  4\n",
    "    }\n",
    "    return blosum62[(str(AA1).upper(), str(AA2).upper())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blosum62('A','Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Sequence: [('sequence_id', 'MVLSPADKTNVKAAWGKVGAHAGEYGAEALERXFLSFPT')]\n",
      "Masked Index: 31\n",
      "Predicted Amino Acid: R\n",
      "Predicted Amino Acid: R\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from esm import pretrained  # Import ESM pretrained models\n",
    "\n",
    "# Define the `mask` function\n",
    "def mask(sequence, masked_index):\n",
    "    \"\"\"\n",
    "    Replaces a character at the specified index with a mask token ('X').\n",
    "    \n",
    "    Parameters:\n",
    "        sequence (str): The original sequence.\n",
    "        masked_index (int): The index of the character to mask.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: Each tuple is a sequence identifier and sequence (required by batch_converter).\n",
    "    \"\"\"\n",
    "    masked_sequence = sequence[:masked_index] + 'X' + sequence[masked_index + 1:]\n",
    "    return [(\"sequence_id\", masked_sequence)]\n",
    "\n",
    "# Main test function\n",
    "def test(sequence, model_loader):\n",
    "    \"\"\"\n",
    "    Tests a protein sequence using a pretrained ESM model and predicts the masked amino acid.\n",
    "    \n",
    "    Parameters:\n",
    "        sequence (str): Input protein sequence.\n",
    "        model_loader (function): Function to load the pretrained model (e.g., `pretrained.esm2_t30_150M_UR50D`).\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Masked index, predicted amino acid, and model results.\n",
    "    \"\"\"\n",
    "    # Randomly mask an index in the sequence\n",
    "    masked_index = np.random.randint(0, len(sequence))+1\n",
    "    masked_sequence = mask(sequence, masked_index)\n",
    "\n",
    "    # Load the model and alphabet\n",
    "    model, alphabet = model_loader()\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Tokenize the masked sequence\n",
    "    print(\"Masked Sequence:\", masked_sequence)\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(masked_sequence)\n",
    "    batch_tokens = batch_tokens.to(device)\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens)\n",
    "    \n",
    "    # Extract logits for the masked position\n",
    "    logits = results[\"logits\"]  # Shape: (batch_size, seq_len, vocab_size)\n",
    "    masked_logits = logits[0, masked_index, :]  # Select the masked token's logits\n",
    "    \n",
    "    # Find the predicted amino acid\n",
    "    predicted_token_id = masked_logits.argmax().item()\n",
    "    predicted_token = alphabet.get_tok(predicted_token_id)\n",
    "\n",
    "    # Return the masked index, predicted amino acid, and full results\n",
    "    return masked_index-1, predicted_token, results\n",
    "\n",
    "# Example usage with pretrained ESM model\n",
    "seq=\"MVLSPADKTNVKAAWGKVGAHAGEYGAEALERMFLSFPT\"\n",
    "masked_index, predicted_aa, results = test(seq, pretrained.esm2_t30_150M_UR50D)\n",
    "print(f\"Masked Index: {masked_index}\")\n",
    "print(f\"Predicted Amino Acid: {predicted_aa}\")\n",
    "print(f\"Predicted Amino Acid: {seq[masked_index]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESM single sequence (esm2_t30_150M_UR50D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from esm import pretrained\n",
    "import utils\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masking residues:\n",
      "MVLSPADKTNVKAAWGKVGAHAGEYGAEALERMFLSFPT\n",
      "MVLSXXXXTXVKAAWGKVGAHAGEYGAEALERMFLSFPT\n",
      "----^^^^-^-----------------------------\n"
     ]
    }
   ],
   "source": [
    "sequence=\"MVLSPADKTNVKAAWGKVGAHAGEYGAEALERMFLSFPT\"\n",
    "model, alphabet = pretrained.esm2_t30_150M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "masked_sequence=utils.mask(sequence, [4,5,6,7,9])\n",
    "print(f'Masking residues:\\n{sequence}\\n{masked_sequence}\\n{\"\".join([\"-\" if sequence[i]==masked_sequence[i] else \"^\" for i in range(len(sequence))])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVLSXXXXTXVKAAWGKVGAHAGEYGAEALERMFLSFPT\n"
     ]
    }
   ],
   "source": [
    "batch_labels, batch_strs, batch_tokens = batch_converter([('ID',masked_sequence)])\n",
    "batch_tokens = batch_tokens.to(device)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    results = model(batch_tokens)\n",
    "print(sequence)\n",
    "print(''.join([alphabet.get_tok(item) for item in results[\"logits\"][0].argmax(dim=-1)[1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 20,  7,  4,  8,  5,  5, 13,  4, 11,  5,  7, 15,  5,  5, 22,  6, 15,\n",
       "         7,  6,  5, 21,  5,  6,  9, 19,  6,  5,  9,  5,  4,  9, 10, 20, 18,  4,\n",
       "         8, 18, 14, 11,  2])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"logits\"][0].argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Index: 13\n",
      "Predicted Amino Acid: A\n",
      "Original Amino Acid: A\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from esm import pretrained  # Import ESM pretrained models\n",
    "\n",
    "# Initialize model and batch_converter globally for reuse\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, alphabet = pretrained.esm2_t30_150M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model = model.to(device)\n",
    "model.eval()  # Switch to evaluation mode\n",
    "\n",
    "\n",
    "def mask(sequence, masked_index):\n",
    "    \"\"\"\n",
    "    Replaces a character at the specified index with a mask token ('X').\n",
    "\n",
    "    Parameters:\n",
    "        sequence (str): The original sequence.\n",
    "        masked_index (int): The index of the character to mask.\n",
    "\n",
    "    Returns:\n",
    "        str: Masked sequence.\n",
    "    \"\"\"\n",
    "    return sequence[:masked_index] + 'X' + sequence[masked_index + 1:]\n",
    "\n",
    "\n",
    "def test(sequence,model,alphabet):\n",
    "    \"\"\"\n",
    "    Tests a protein sequence using a pretrained ESM model and predicts the masked amino acid.\n",
    "\n",
    "    Parameters:\n",
    "        sequence (str): Input protein sequence.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Masked index, predicted amino acid, and original amino acid.\n",
    "    \"\"\"\n",
    "    # Randomly mask an index in the sequence\n",
    "    masked_index = np.random.randint(1, len(sequence))+1  # Masking starts from index 0\n",
    "\n",
    "    # Tokenize the masked sequence\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter([(\"sequence_id\", mask(sequence, masked_index))])\n",
    "    batch_tokens = batch_tokens.to(device)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens)\n",
    "\n",
    "    # Return masked index, predicted amino acid, and original amino acid\n",
    "    return masked_index-1, alphabet.get_tok(results[\"logits\"][0, masked_index, :].argmax().item()), sequence[masked_index-1]\n",
    "\n",
    "\n",
    "# Example usage\n",
    "seq = \"MVLSPADKTNVKAAWGKVGAHAGEYGAEALERMFLSFPT\"\n",
    "masked_index, predicted_aa, label = test(seq,model,alphabet)\n",
    "print(f\"Masked Index: {masked_index}\")\n",
    "print(f\"Predicted Amino Acid: {predicted_aa}\")\n",
    "print(f\"Original Amino Acid: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/938 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [28:40<00:00,  1.83s/it]  \n"
     ]
    }
   ],
   "source": [
    "def read_fasta(file_path):\n",
    "    sequences = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if not line.startswith(\">\"):\n",
    "                sequences.append(line.strip())\n",
    "    return sequences\n",
    "\n",
    "results=[]\n",
    "for seq in tqdm(read_fasta('Project2/BindingDBTargetSequences.fasta')[::10]):\n",
    "    results.append(test(seq,model,alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(329, 'D', 'D'),\n",
       " (27, 'E', 'E'),\n",
       " (67, 'G', 'G'),\n",
       " (312, 'P', 'P'),\n",
       " (41, 'I', 'I'),\n",
       " (52, 'E', 'E'),\n",
       " (44, 'N', 'N'),\n",
       " (461, 'P', 'P'),\n",
       " (177, 'A', 'A'),\n",
       " (12, 'A', 'A'),\n",
       " (91, 'Q', 'Q'),\n",
       " (478, 'A', 'A'),\n",
       " (491, 'E', 'E'),\n",
       " (118, 'A', 'A'),\n",
       " (98, 'T', 'T'),\n",
       " (53, 'I', 'I'),\n",
       " (67, 'G', 'G'),\n",
       " (323, 'T', 'T'),\n",
       " (77, 'Q', 'Q'),\n",
       " (344, 'Q', 'Q'),\n",
       " (147, 'G', 'G'),\n",
       " (190, 'K', 'K'),\n",
       " (160, 'R', 'R'),\n",
       " (68, 'V', 'V'),\n",
       " (482, 'Y', 'Y'),\n",
       " (278, 'L', 'L'),\n",
       " (576, 'L', 'L'),\n",
       " (1063, 'E', 'E'),\n",
       " (56, 'L', 'L'),\n",
       " (24, 'D', 'D'),\n",
       " (41, 'G', 'G'),\n",
       " (333, 'S', 'S'),\n",
       " (157, 'Q', 'Q'),\n",
       " (5, 'L', 'L'),\n",
       " (410, 'N', 'N'),\n",
       " (51, 'S', 'S'),\n",
       " (494, 'D', 'D'),\n",
       " (86, 'A', 'A'),\n",
       " (196, 'Q', 'Q'),\n",
       " (318, 'L', 'L'),\n",
       " (113, 'Y', 'Y'),\n",
       " (554, 'L', 'L'),\n",
       " (61, 'Y', 'Y'),\n",
       " (11, 'S', 'S'),\n",
       " (696, 'E', 'E'),\n",
       " (89, 'V', 'V'),\n",
       " (467, 'R', 'R'),\n",
       " (499, 'L', 'L'),\n",
       " (534, 'N', 'N'),\n",
       " (113, 'E', 'E'),\n",
       " (964, 'I', 'I'),\n",
       " (73, 'V', 'V'),\n",
       " (327, 'L', 'L'),\n",
       " (505, 'P', 'P'),\n",
       " (159, 'S', 'S'),\n",
       " (305, 'E', 'E'),\n",
       " (282, 'E', 'E'),\n",
       " (31, 'V', 'V'),\n",
       " (31, 'V', 'V'),\n",
       " (141, 'Q', 'Q'),\n",
       " (90, 'T', 'T'),\n",
       " (343, 'C', 'C'),\n",
       " (171, 'N', 'N'),\n",
       " (590, 'R', 'R'),\n",
       " (444, 'Y', 'Y'),\n",
       " (256, 'D', 'D'),\n",
       " (3467, 'S', 'S'),\n",
       " (522, 'D', 'D'),\n",
       " (378, 'F', 'F'),\n",
       " (511, 'E', 'E'),\n",
       " (274, 'P', 'P'),\n",
       " (23, 'P', 'P'),\n",
       " (254, 'V', 'V'),\n",
       " (9, 'G', 'G'),\n",
       " (257, 'G', 'G'),\n",
       " (12, 'Y', 'Y'),\n",
       " (257, 'T', 'T'),\n",
       " (108, 'E', 'E'),\n",
       " (420, 'T', 'T'),\n",
       " (164, 'E', 'E'),\n",
       " (683, 'K', 'K'),\n",
       " (178, 'S', 'S'),\n",
       " (69, 'Y', 'Y'),\n",
       " (57, 'Q', 'Q'),\n",
       " (186, 'K', 'K'),\n",
       " (435, 'I', 'I'),\n",
       " (311, 'A', 'A'),\n",
       " (125, 'R', 'R'),\n",
       " (325, 'L', 'L'),\n",
       " (37, 'V', 'V'),\n",
       " (81, 'V', 'V'),\n",
       " (187, 'V', 'V'),\n",
       " (518, 'N', 'N'),\n",
       " (87, 'I', 'I'),\n",
       " (66, 'G', 'G'),\n",
       " (431, 'V', 'V'),\n",
       " (62, 'R', 'W'),\n",
       " (84, 'L', 'L'),\n",
       " (75, 'F', 'F'),\n",
       " (287, 'I', 'I'),\n",
       " (247, 'V', 'V'),\n",
       " (148, 'V', 'V'),\n",
       " (287, 'K', 'K'),\n",
       " (691, 'L', 'L'),\n",
       " (99, 'L', 'L'),\n",
       " (494, 'R', 'R'),\n",
       " (115, 'L', 'L'),\n",
       " (235, 'Y', 'Y'),\n",
       " (187, 'S', 'S'),\n",
       " (103, 'A', 'A'),\n",
       " (156, 'I', 'I'),\n",
       " (101, 'V', 'V'),\n",
       " (54, 'D', 'D'),\n",
       " (76, 'V', 'V'),\n",
       " (9, 'P', 'P'),\n",
       " (172, 'R', 'R'),\n",
       " (81, 'E', 'E'),\n",
       " (145, 'A', 'A'),\n",
       " (368, 'K', 'K'),\n",
       " (164, 'V', 'V'),\n",
       " (426, 'I', 'I'),\n",
       " (144, 'A', 'A'),\n",
       " (305, 'P', 'P'),\n",
       " (206, 'N', 'N'),\n",
       " (58, 'R', 'R'),\n",
       " (6, 'A', 'A'),\n",
       " (151, 'V', 'V'),\n",
       " (176, 'A', 'A'),\n",
       " (2564, 'Q', 'Q'),\n",
       " (880, 'M', 'M'),\n",
       " (1947, 'I', 'I'),\n",
       " (105, 'L', 'L'),\n",
       " (197, 'R', 'R'),\n",
       " (218, 'P', 'P'),\n",
       " (558, 'A', 'A'),\n",
       " (189, 'V', 'V'),\n",
       " (262, 'E', 'E'),\n",
       " (269, 'S', 'S'),\n",
       " (340, 'I', 'I'),\n",
       " (523, 'V', 'V'),\n",
       " (430, 'V', 'V'),\n",
       " (241, 'G', 'G'),\n",
       " (250, 'L', 'L'),\n",
       " (765, 'Q', 'Q'),\n",
       " (26, 'Q', 'Q'),\n",
       " (42, 'R', 'R'),\n",
       " (143, 'E', 'E'),\n",
       " (485, 'R', 'R'),\n",
       " (217, 'W', 'W'),\n",
       " (94, 'G', 'G'),\n",
       " (321, 'T', 'T'),\n",
       " (73, 'M', 'L'),\n",
       " (324, 'A', 'A'),\n",
       " (345, 'T', 'T'),\n",
       " (59, 'H', 'H'),\n",
       " (224, 'M', 'M'),\n",
       " (242, 'L', 'L'),\n",
       " (1172, 'L', 'L'),\n",
       " (350, 'N', 'N'),\n",
       " (685, 'I', 'I'),\n",
       " (211, 'G', 'G'),\n",
       " (103, 'K', 'K'),\n",
       " (66, 'R', 'R'),\n",
       " (488, 'V', 'V'),\n",
       " (281, 'R', 'R'),\n",
       " (204, 'K', 'K'),\n",
       " (179, 'T', 'T'),\n",
       " (115, 'E', 'E'),\n",
       " (75, 'T', 'T'),\n",
       " (269, 'F', 'F'),\n",
       " (226, 'S', 'S'),\n",
       " (88, 'R', 'R'),\n",
       " (190, 'A', 'A'),\n",
       " (21, 'P', 'P'),\n",
       " (648, 'E', 'E'),\n",
       " (774, 'A', 'A'),\n",
       " (622, 'G', 'G'),\n",
       " (57, 'A', 'A'),\n",
       " (359, 'S', 'S'),\n",
       " (144, 'A', 'A'),\n",
       " (77, 'W', 'W'),\n",
       " (397, 'I', 'I'),\n",
       " (226, 'F', 'F'),\n",
       " (206, 'K', 'K'),\n",
       " (157, 'L', 'L'),\n",
       " (47, 'G', 'G'),\n",
       " (195, 'G', 'G'),\n",
       " (18, 'E', 'E'),\n",
       " (85, 'L', 'L'),\n",
       " (521, 'A', 'A'),\n",
       " (381, 'K', 'K'),\n",
       " (279, 'L', 'L'),\n",
       " (232, 'L', 'L'),\n",
       " (55, 'D', 'D'),\n",
       " (346, 'E', 'E'),\n",
       " (372, 'N', 'N'),\n",
       " (123, 'R', 'R'),\n",
       " (26, 'Q', 'Q'),\n",
       " (78, 'Y', 'Y'),\n",
       " (98, 'G', 'G'),\n",
       " (27, 'Y', 'Y'),\n",
       " (251, 'E', 'E'),\n",
       " (2004, 'P', 'P'),\n",
       " (111, 'L', 'L'),\n",
       " (436, 'E', 'E'),\n",
       " (238, 'D', 'D'),\n",
       " (231, 'V', 'V'),\n",
       " (364, 'T', 'T'),\n",
       " (216, 'A', 'A'),\n",
       " (121, 'A', 'A'),\n",
       " (108, 'L', 'L'),\n",
       " (382, 'I', 'I'),\n",
       " (150, 'D', 'D'),\n",
       " (164, 'E', 'E'),\n",
       " (295, 'H', 'H'),\n",
       " (329, 'T', 'T'),\n",
       " (87, 'N', 'N'),\n",
       " (222, 'T', 'T'),\n",
       " (34, 'N', 'N'),\n",
       " (133, 'N', 'N'),\n",
       " (397, 'H', 'H'),\n",
       " (967, 'S', 'S'),\n",
       " (141, 'E', 'E'),\n",
       " (368, 'I', 'I'),\n",
       " (254, 'L', 'L'),\n",
       " (1134, 'R', 'R'),\n",
       " (1121, 'D', 'D'),\n",
       " (411, 'V', 'V'),\n",
       " (49, 'H', 'H'),\n",
       " (128, 'R', 'R'),\n",
       " (8, 'R', 'R'),\n",
       " (175, 'Q', 'Q'),\n",
       " (68, 'H', 'H'),\n",
       " (27, 'S', 'S'),\n",
       " (675, 'K', 'K'),\n",
       " (86, 'L', 'L'),\n",
       " (602, 'G', 'G'),\n",
       " (1, 'S', 'S'),\n",
       " (169, 'E', 'E'),\n",
       " (754, 'H', 'H'),\n",
       " (248, 'I', 'I'),\n",
       " (55, 'K', 'K'),\n",
       " (116, 'L', 'L'),\n",
       " (14, 'G', 'G'),\n",
       " (245, 'S', 'S'),\n",
       " (324, 'L', 'L'),\n",
       " (1, 'A', 'A'),\n",
       " (474, 'P', 'P'),\n",
       " (208, 'T', 'T'),\n",
       " (83, 'D', 'D'),\n",
       " (272, 'C', 'C'),\n",
       " (140, 'R', 'R'),\n",
       " (130, 'T', 'T'),\n",
       " (522, 'S', 'S'),\n",
       " (473, 'V', 'V'),\n",
       " (3, 'A', 'A'),\n",
       " (172, 'G', 'G'),\n",
       " (290, 'D', 'D'),\n",
       " (254, 'I', 'I'),\n",
       " (382, 'R', 'R'),\n",
       " (341, 'I', 'I'),\n",
       " (9, 'T', 'T'),\n",
       " (123, 'K', 'K'),\n",
       " (84, 'L', 'L'),\n",
       " (178, 'N', 'N'),\n",
       " (31, 'G', 'G'),\n",
       " (96, 'L', 'L'),\n",
       " (214, 'V', 'V'),\n",
       " (54, 'S', 'S'),\n",
       " (39, 'G', 'G'),\n",
       " (85, 'K', 'K'),\n",
       " (75, 'G', 'G'),\n",
       " (88, 'Y', 'Y'),\n",
       " (344, 'R', 'R'),\n",
       " (175, 'R', 'R'),\n",
       " (50, 'E', 'E'),\n",
       " (65, 'L', 'L'),\n",
       " (108, 'S', 'S'),\n",
       " (394, 'I', 'I'),\n",
       " (242, 'V', 'V'),\n",
       " (131, 'C', 'C'),\n",
       " (216, 'K', 'K'),\n",
       " (671, 'E', 'E'),\n",
       " (178, 'G', 'G'),\n",
       " (29, 'G', 'G'),\n",
       " (1857, 'S', 'S'),\n",
       " (236, 'G', 'G'),\n",
       " (379, 'S', 'S'),\n",
       " (2, 'E', 'E'),\n",
       " (81, 'S', 'S'),\n",
       " (217, 'E', 'E'),\n",
       " (626, 'K', 'K'),\n",
       " (160, 'H', 'H'),\n",
       " (307, 'L', 'L'),\n",
       " (98, 'D', 'D'),\n",
       " (217, 'C', 'C'),\n",
       " (313, 'C', 'C'),\n",
       " (696, 'N', 'N'),\n",
       " (141, 'T', 'T'),\n",
       " (221, 'D', 'D'),\n",
       " (43, 'M', 'M'),\n",
       " (135, 'L', 'L'),\n",
       " (59, 'Y', 'Y'),\n",
       " (39, 'S', 'S'),\n",
       " (92, 'T', 'T'),\n",
       " (323, 'H', 'H'),\n",
       " (282, 'D', 'D'),\n",
       " (569, 'Y', 'Y'),\n",
       " (567, 'C', 'C'),\n",
       " (282, 'L', 'L'),\n",
       " (327, 'T', 'T'),\n",
       " (8, 'A', 'A'),\n",
       " (125, 'I', 'I'),\n",
       " (54, 'V', 'V'),\n",
       " (96, 'T', 'T'),\n",
       " (418, 'Y', 'Y'),\n",
       " (277, 'L', 'L'),\n",
       " (140, 'K', 'K'),\n",
       " (511, 'G', 'G'),\n",
       " (228, 'S', 'S'),\n",
       " (452, 'S', 'S'),\n",
       " (342, 'E', 'E'),\n",
       " (324, 'A', 'Q'),\n",
       " (52, 'A', 'A'),\n",
       " (83, 'E', 'E'),\n",
       " (7, 'L', 'L'),\n",
       " (623, 'S', 'S'),\n",
       " (71, 'K', 'K'),\n",
       " (143, 'F', 'F'),\n",
       " (21, 'D', 'D'),\n",
       " (339, 'Q', 'Q'),\n",
       " (852, 'S', 'S'),\n",
       " (378, 'Q', 'Q'),\n",
       " (144, 'L', 'L'),\n",
       " (264, 'C', 'C'),\n",
       " (343, 'M', 'M'),\n",
       " (236, 'E', 'E'),\n",
       " (179, 'P', 'P'),\n",
       " (351, 'D', 'D'),\n",
       " (100, 'W', 'W'),\n",
       " (597, 'D', 'D'),\n",
       " (271, 'E', 'E'),\n",
       " (54, 'S', 'S'),\n",
       " (38, 'G', 'G'),\n",
       " (74, 'I', 'I'),\n",
       " (435, 'E', 'E'),\n",
       " (375, 'L', 'L'),\n",
       " (738, 'L', 'L'),\n",
       " (72, 'N', 'N'),\n",
       " (258, 'L', 'L'),\n",
       " (275, 'V', 'V'),\n",
       " (211, 'L', 'L'),\n",
       " (128, 'I', 'I'),\n",
       " (88, 'L', 'L'),\n",
       " (914, 'L', 'L'),\n",
       " (226, 'V', 'V'),\n",
       " (161, 'W', 'W'),\n",
       " (2158, 'Q', 'Q'),\n",
       " (64, 'L', 'L'),\n",
       " (42, 'Q', 'Q'),\n",
       " (529, 'F', 'F'),\n",
       " (126, 'T', 'T'),\n",
       " (206, 'F', 'F'),\n",
       " (158, 'E', 'E'),\n",
       " (33, 'N', 'N'),\n",
       " (25, 'C', 'C'),\n",
       " (364, 'L', 'L'),\n",
       " (116, 'K', 'K'),\n",
       " (159, 'N', 'N'),\n",
       " (363, 'Q', 'Q'),\n",
       " (11, 'Q', 'Q'),\n",
       " (61, 'E', 'E'),\n",
       " (43, 'V', 'V'),\n",
       " (58, 'Y', 'Y'),\n",
       " (16, 'S', 'S'),\n",
       " (266, 'P', 'P'),\n",
       " (166, 'W', 'W'),\n",
       " (75, 'K', 'K'),\n",
       " (214, 'G', 'G'),\n",
       " (237, 'F', 'F'),\n",
       " (154, 'V', 'V'),\n",
       " (751, 'L', 'L'),\n",
       " (33, 'D', 'D'),\n",
       " (535, 'E', 'E'),\n",
       " (279, 'G', 'G'),\n",
       " (66, 'E', 'E'),\n",
       " (379, 'F', 'F'),\n",
       " (570, 'D', 'D'),\n",
       " (111, 'P', 'P'),\n",
       " (39, 'K', 'K'),\n",
       " (137, 'Q', 'Q'),\n",
       " (257, 'S', 'S'),\n",
       " (386, 'L', 'L'),\n",
       " (151, 'G', 'G'),\n",
       " (185, 'E', 'E'),\n",
       " (145, 'A', 'A'),\n",
       " (265, 'L', 'L'),\n",
       " (283, 'A', 'A'),\n",
       " (233, 'G', 'G'),\n",
       " (472, 'F', 'F'),\n",
       " (162, 'D', 'D'),\n",
       " (209, 'W', 'W'),\n",
       " (317, 'A', 'A'),\n",
       " (441, 'F', 'F'),\n",
       " (625, 'I', 'I'),\n",
       " (388, 'T', 'T'),\n",
       " (225, 'N', 'N'),\n",
       " (24, 'P', 'P'),\n",
       " (333, 'L', 'L'),\n",
       " (123, 'R', 'R'),\n",
       " (236, 'A', 'A'),\n",
       " (82, 'G', 'G'),\n",
       " (479, 'P', 'P'),\n",
       " (55, 'S', 'S'),\n",
       " (375, 'L', 'L'),\n",
       " (315, 'M', 'M'),\n",
       " (30, 'V', 'V'),\n",
       " (353, 'E', 'E'),\n",
       " (471, 'F', 'F'),\n",
       " (52, 'V', 'V'),\n",
       " (3, 'L', 'L'),\n",
       " (457, 'H', 'H'),\n",
       " (196, 'T', 'T'),\n",
       " (65, 'A', 'A'),\n",
       " (355, 'G', 'G'),\n",
       " (113, 'D', 'D'),\n",
       " (31, 'L', 'L'),\n",
       " (75, 'G', 'G'),\n",
       " (323, 'A', 'A'),\n",
       " (229, 'K', 'K'),\n",
       " (136, 'E', 'E'),\n",
       " (587, 'E', 'E'),\n",
       " (189, 'N', 'N'),\n",
       " (8, 'S', 'S'),\n",
       " (755, 'R', 'R'),\n",
       " (94, 'L', 'L'),\n",
       " (131, 'V', 'V'),\n",
       " (807, 'Q', 'Q'),\n",
       " (295, 'A', 'A'),\n",
       " (267, 'H', 'H'),\n",
       " (315, 'Y', 'Y'),\n",
       " (215, 'V', 'V'),\n",
       " (223, 'T', 'T'),\n",
       " (280, 'M', 'M'),\n",
       " (184, 'R', 'R'),\n",
       " (664, 'G', 'G'),\n",
       " (476, 'N', 'N'),\n",
       " (32, 'S', 'S'),\n",
       " (336, 'V', 'V'),\n",
       " (63, 'T', 'A'),\n",
       " (185, 'P', 'P'),\n",
       " (144, 'R', 'R'),\n",
       " (16, 'P', 'P'),\n",
       " (275, 'V', 'V'),\n",
       " (270, 'F', 'F'),\n",
       " (16, 'S', 'S'),\n",
       " (1218, 'N', 'N'),\n",
       " (38, 'K', 'K'),\n",
       " (254, 'A', 'A'),\n",
       " (146, 'P', 'P'),\n",
       " (434, 'K', 'K'),\n",
       " (1000, 'R', 'R'),\n",
       " (225, 'S', 'S'),\n",
       " (80, 'P', 'P'),\n",
       " (139, 'C', 'C'),\n",
       " (307, 'H', 'H'),\n",
       " (140, 'S', 'S'),\n",
       " (361, 'D', 'D'),\n",
       " (9, 'A', 'A'),\n",
       " (7, 'A', 'A'),\n",
       " (187, 'N', 'N'),\n",
       " (46, 'P', 'P'),\n",
       " (160, 'E', 'E'),\n",
       " (118, 'S', 'S'),\n",
       " (435, 'L', 'L'),\n",
       " (21, 'R', 'R'),\n",
       " (664, 'V', 'V'),\n",
       " (487, 'E', 'E'),\n",
       " (148, 'D', 'D'),\n",
       " (58, 'I', 'I'),\n",
       " (211, 'Y', 'Y'),\n",
       " (28, 'V', 'V'),\n",
       " (293, 'L', 'L'),\n",
       " (48, 'E', 'E'),\n",
       " (198, 'F', 'F'),\n",
       " (318, 'L', 'L'),\n",
       " (218, 'L', 'L'),\n",
       " (281, 'E', 'E'),\n",
       " (40, 'F', 'F'),\n",
       " (47, 'T', 'T'),\n",
       " (244, 'F', 'F'),\n",
       " (932, 'L', 'L'),\n",
       " (72, 'G', 'G'),\n",
       " (91, 'S', 'S'),\n",
       " (219, 'N', 'N'),\n",
       " (141, 'R', 'R'),\n",
       " (37, 'R', 'R'),\n",
       " (56, 'K', 'K'),\n",
       " (78, 'S', 'I'),\n",
       " (360, 'G', 'G'),\n",
       " (82, 'G', 'G'),\n",
       " (90, 'W', 'W'),\n",
       " (14, 'Q', 'Q'),\n",
       " (328, 'N', 'N'),\n",
       " (176, 'D', 'A'),\n",
       " (164, 'I', 'I'),\n",
       " (187, 'L', 'L'),\n",
       " (147, 'A', 'A'),\n",
       " (224, 'P', 'P'),\n",
       " (56, 'K', 'K'),\n",
       " (349, 'S', 'S'),\n",
       " (28, 'P', 'P'),\n",
       " (182, 'I', 'I'),\n",
       " (344, 'V', 'V'),\n",
       " (345, 'G', 'G'),\n",
       " (437, 'W', 'W'),\n",
       " (108, 'S', 'S'),\n",
       " (363, 'L', 'L'),\n",
       " (11, 'L', 'L'),\n",
       " (340, 'L', 'L'),\n",
       " (329, 'E', 'E'),\n",
       " (271, 'N', 'N'),\n",
       " (183, 'R', 'R'),\n",
       " (280, 'I', 'I'),\n",
       " (167, 'L', 'L'),\n",
       " (17, 'E', 'E'),\n",
       " (299, 'V', 'V'),\n",
       " (286, 'I', 'I'),\n",
       " (242, 'T', 'T'),\n",
       " (255, 'R', 'R'),\n",
       " (150, 'A', 'A'),\n",
       " (34, 'P', 'P'),\n",
       " (483, 'V', 'V'),\n",
       " (397, 'M', 'M'),\n",
       " (204, 'Q', 'Q'),\n",
       " (78, 'P', 'P'),\n",
       " (210, 'L', 'L'),\n",
       " (630, 'N', 'N'),\n",
       " (170, 'G', 'G'),\n",
       " (28, 'S', 'S'),\n",
       " (81, 'D', 'D'),\n",
       " (299, 'F', 'F'),\n",
       " (57, 'E', 'E'),\n",
       " (317, 'T', 'T'),\n",
       " (446, 'R', 'R'),\n",
       " (525, 'G', 'G'),\n",
       " (813, 'F', 'F'),\n",
       " (199, 'W', 'W'),\n",
       " (223, 'P', 'P'),\n",
       " (344, 'C', 'C'),\n",
       " (273, 'L', 'L'),\n",
       " (484, 'N', 'N'),\n",
       " (491, 'H', 'H'),\n",
       " (448, 'D', 'D'),\n",
       " (673, 'L', 'L'),\n",
       " (212, 'L', 'L'),\n",
       " (701, 'R', 'R'),\n",
       " (159, 'A', 'A'),\n",
       " (1231, 'N', 'N'),\n",
       " (718, 'D', 'D'),\n",
       " (545, 'R', 'R'),\n",
       " (14, 'E', 'E'),\n",
       " (38, 'T', 'T'),\n",
       " (140, 'K', 'K'),\n",
       " (116, 'A', 'A'),\n",
       " (115, 'R', 'R'),\n",
       " (109, 'Y', 'Y'),\n",
       " (14, 'G', 'G'),\n",
       " (389, 'S', 'S'),\n",
       " (17, 'K', 'K'),\n",
       " (189, 'G', 'G'),\n",
       " (161, 'T', 'T'),\n",
       " (233, 'I', 'I'),\n",
       " (347, 'S', 'S'),\n",
       " (733, 'I', 'I'),\n",
       " (945, 'T', 'T'),\n",
       " (77, 'L', 'L'),\n",
       " (112, 'A', 'A'),\n",
       " (209, 'R', 'R'),\n",
       " (2173, 'R', 'R'),\n",
       " (171, 'A', 'A'),\n",
       " (160, 'V', 'V'),\n",
       " (406, 'E', 'E'),\n",
       " (297, 'W', 'W'),\n",
       " (107, 'G', 'G'),\n",
       " (569, 'T', 'T'),\n",
       " (33, 'G', 'G'),\n",
       " (16, 'E', 'E'),\n",
       " (244, 'H', 'H'),\n",
       " (93, 'Y', 'Y'),\n",
       " (241, 'Q', 'Q'),\n",
       " (188, 'G', 'G'),\n",
       " (17, 'R', 'R'),\n",
       " (744, 'E', 'E'),\n",
       " (207, 'G', 'G'),\n",
       " (52, 'V', 'V'),\n",
       " (577, 'F', 'F'),\n",
       " (219, 'S', 'S'),\n",
       " (49, 'Y', 'Y'),\n",
       " (405, 'C', 'C'),\n",
       " (100, 'T', 'T'),\n",
       " (112, 'F', 'F'),\n",
       " (255, 'F', 'F'),\n",
       " (146, 'I', 'I'),\n",
       " (278, 'K', 'K'),\n",
       " (219, 'N', 'N'),\n",
       " (58, 'G', 'G'),\n",
       " (421, 'S', 'S'),\n",
       " (318, 'D', 'D'),\n",
       " (45, 'F', 'F'),\n",
       " (240, 'I', 'I'),\n",
       " (71, 'N', 'N'),\n",
       " (609, 'P', 'P'),\n",
       " (194, 'L', 'L'),\n",
       " (642, 'T', 'T'),\n",
       " (553, 'S', 'S'),\n",
       " (485, 'V', 'V'),\n",
       " (243, 'I', 'I'),\n",
       " (314, 'G', 'G'),\n",
       " (2053, 'Y', 'Y'),\n",
       " (453, 'K', 'K'),\n",
       " (94, 'V', 'V'),\n",
       " (492, 'L', 'L'),\n",
       " (1338, 'P', 'P'),\n",
       " (139, 'Y', 'Y'),\n",
       " (270, 'K', 'K'),\n",
       " (195, 'I', 'T'),\n",
       " (593, 'W', 'W'),\n",
       " (167, 'W', 'W'),\n",
       " (321, 'R', 'R'),\n",
       " (150, 'A', 'A'),\n",
       " (487, 'F', 'F'),\n",
       " (370, 'D', 'D'),\n",
       " (240, 'R', 'R'),\n",
       " (362, 'A', 'A'),\n",
       " (244, 'M', 'M'),\n",
       " (184, 'E', 'E'),\n",
       " (14, 'L', 'L'),\n",
       " (175, 'Y', 'Y'),\n",
       " (366, 'D', 'D'),\n",
       " (192, 'L', 'L'),\n",
       " (144, 'M', 'M'),\n",
       " (168, 'T', 'T'),\n",
       " (525, 'V', 'V'),\n",
       " (237, 'S', 'S'),\n",
       " (240, 'G', 'G'),\n",
       " (829, 'L', 'L'),\n",
       " (825, 'Q', 'Q'),\n",
       " (72, 'K', 'K'),\n",
       " (55, 'R', 'R'),\n",
       " (181, 'A', 'A'),\n",
       " (121, 'K', 'K'),\n",
       " (111, 'Y', 'Y'),\n",
       " (173, 'V', 'V'),\n",
       " (674, 'K', 'K'),\n",
       " (67, 'F', 'F'),\n",
       " (108, 'E', 'N'),\n",
       " (192, 'E', 'E'),\n",
       " (346, 'N', 'N'),\n",
       " (397, 'A', 'A'),\n",
       " (63, 'F', 'F'),\n",
       " (219, 'I', 'I'),\n",
       " (766, 'D', 'D'),\n",
       " (353, 'Q', 'Q'),\n",
       " (124, 'P', 'P'),\n",
       " (157, 'L', 'L'),\n",
       " (109, 'F', 'F'),\n",
       " (82, 'I', 'I'),\n",
       " (198, 'K', 'K'),\n",
       " (50, 'I', 'I'),\n",
       " (153, 'R', 'R'),\n",
       " (129, 'V', 'V'),\n",
       " (116, 'A', 'A'),\n",
       " (345, 'A', 'A'),\n",
       " (186, 'S', 'S'),\n",
       " (362, 'S', 'S'),\n",
       " (339, 'D', 'D'),\n",
       " (405, 'I', 'I'),\n",
       " (244, 'A', 'A'),\n",
       " (126, 'L', 'L'),\n",
       " (821, 'G', 'G'),\n",
       " (388, 'D', 'D'),\n",
       " (64, 'N', 'N'),\n",
       " (89, 'S', 'S'),\n",
       " (36, 'V', 'V'),\n",
       " (94, 'V', 'V'),\n",
       " (390, 'S', 'S'),\n",
       " (86, 'F', 'F'),\n",
       " (175, 'L', 'L'),\n",
       " (381, 'S', 'S'),\n",
       " (371, 'K', 'K'),\n",
       " (158, 'A', 'A'),\n",
       " (2254, 'A', 'A'),\n",
       " (459, 'F', 'F'),\n",
       " (31, 'L', 'L'),\n",
       " (742, 'V', 'V'),\n",
       " (459, 'N', 'N'),\n",
       " (220, 'I', 'I'),\n",
       " (448, 'I', 'I'),\n",
       " (707, 'F', 'F'),\n",
       " (417, 'P', 'P'),\n",
       " (76, 'H', 'H'),\n",
       " (307, 'Y', 'Y'),\n",
       " (385, 'S', 'S'),\n",
       " (255, 'I', 'I'),\n",
       " (383, 'G', 'G'),\n",
       " (79, 'Q', 'Q'),\n",
       " (29, 'K', 'K'),\n",
       " (345, 'I', 'I'),\n",
       " (75, 'A', 'A'),\n",
       " (225, 'V', 'V'),\n",
       " (1146, 'K', 'K'),\n",
       " (202, 'S', 'S'),\n",
       " (170, 'R', 'R'),\n",
       " (422, 'L', 'L'),\n",
       " (659, 'N', 'N'),\n",
       " (156, 'A', 'A'),\n",
       " (80, 'V', 'V'),\n",
       " (253, 'H', 'H'),\n",
       " (995, 'M', 'M'),\n",
       " (110, 'V', 'V'),\n",
       " (263, 'R', 'R'),\n",
       " (1292, 'R', 'R'),\n",
       " (82, 'E', 'E'),\n",
       " (65, 'F', 'F'),\n",
       " (78, 'I', 'I'),\n",
       " (78, 'V', 'V'),\n",
       " (404, 'H', 'H'),\n",
       " (13, 'H', 'H'),\n",
       " (21, 'G', 'G'),\n",
       " (206, 'T', 'T'),\n",
       " (619, 'E', 'E'),\n",
       " (336, 'L', 'L'),\n",
       " (138, 'T', 'T'),\n",
       " (939, 'E', 'E'),\n",
       " (166, 'M', 'M'),\n",
       " (179, 'G', 'G'),\n",
       " (101, 'A', 'A'),\n",
       " (63, 'T', 'T'),\n",
       " (208, 'S', 'S'),\n",
       " (128, 'L', 'L'),\n",
       " (329, 'D', 'D'),\n",
       " (513, 'H', 'H'),\n",
       " (169, 'R', 'R'),\n",
       " (265, 'I', 'I'),\n",
       " (757, 'D', 'D'),\n",
       " (128, 'K', 'T'),\n",
       " (15, 'Q', 'Q'),\n",
       " (52, 'K', 'K'),\n",
       " (103, 'H', 'H'),\n",
       " (191, 'F', 'F'),\n",
       " (17, 'E', 'E'),\n",
       " (102, 'S', 'S'),\n",
       " (393, 'D', 'D'),\n",
       " (90, 'F', 'F'),\n",
       " (590, 'K', 'K'),\n",
       " (959, 'R', 'R'),\n",
       " (12, 'L', 'L'),\n",
       " (24, 'Q', 'Q'),\n",
       " (438, 'R', 'R'),\n",
       " (539, 'T', 'T'),\n",
       " (1426, 'S', 'S'),\n",
       " (214, 'G', 'G'),\n",
       " (204, 'T', 'T'),\n",
       " (175, 'G', 'G'),\n",
       " (118, 'G', 'G'),\n",
       " (41, 'Y', 'Y'),\n",
       " (573, 'M', 'M'),\n",
       " (801, 'R', 'R'),\n",
       " (664, 'L', 'L'),\n",
       " (414, 'A', 'A'),\n",
       " (281, 'P', 'P'),\n",
       " (851, 'Q', 'Q'),\n",
       " (48, 'A', 'A'),\n",
       " (78, 'D', 'D'),\n",
       " (21, 'E', 'E'),\n",
       " (179, 'R', 'R'),\n",
       " (82, 'G', 'G'),\n",
       " (6, 'K', 'K'),\n",
       " (57, 'A', 'A'),\n",
       " (399, 'I', 'I'),\n",
       " (411, 'W', 'W'),\n",
       " (199, 'K', 'K'),\n",
       " (139, 'V', 'V'),\n",
       " (422, 'I', 'I'),\n",
       " (433, 'C', 'C'),\n",
       " (157, 'K', 'V'),\n",
       " (289, 'T', 'T'),\n",
       " (160, 'M', 'M'),\n",
       " (676, 'I', 'I'),\n",
       " (247, 'H', 'H'),\n",
       " (700, 'D', 'D'),\n",
       " (20, 'E', 'E'),\n",
       " (327, 'L', 'L'),\n",
       " (43, 'L', 'L'),\n",
       " (82, 'Q', 'Q'),\n",
       " (197, 'V', 'G'),\n",
       " (7, 'G', 'G'),\n",
       " (271, 'R', 'R'),\n",
       " (67, 'M', 'M'),\n",
       " (261, 'Q', 'Q'),\n",
       " (366, 'C', 'C'),\n",
       " (138, 'H', 'H'),\n",
       " (180, 'M', 'M'),\n",
       " (505, 'Y', 'Y'),\n",
       " (150, 'P', 'P'),\n",
       " (178, 'E', 'E'),\n",
       " (10, 'S', 'S'),\n",
       " (172, 'G', 'G'),\n",
       " (461, 'A', 'A'),\n",
       " (398, 'D', 'D'),\n",
       " (80, 'L', 'C'),\n",
       " (286, 'K', 'K'),\n",
       " (288, 'M', 'M'),\n",
       " (237, 'I', 'I'),\n",
       " (212, 'K', 'K'),\n",
       " (433, 'T', 'T'),\n",
       " (8, 'R', 'R'),\n",
       " (433, 'L', 'L'),\n",
       " (198, 'R', 'R'),\n",
       " (711, 'V', 'V'),\n",
       " (124, 'A', 'A'),\n",
       " (599, 'K', 'K'),\n",
       " (222, 'V', 'V'),\n",
       " (881, 'N', 'N'),\n",
       " (124, 'I', 'I'),\n",
       " (46, 'E', 'E'),\n",
       " (246, 'S', 'S'),\n",
       " (92, 'V', 'V'),\n",
       " (34, 'L', 'L'),\n",
       " (1258, 'M', 'M'),\n",
       " (9, 'V', 'V'),\n",
       " (345, 'P', 'P'),\n",
       " (53, 'P', 'P'),\n",
       " (72, 'Y', 'Y'),\n",
       " (68, 'C', 'C'),\n",
       " (155, 'S', 'S'),\n",
       " (50, 'G', 'G'),\n",
       " (17, 'T', 'T'),\n",
       " (316, 'Q', 'Q'),\n",
       " (360, 'C', 'C'),\n",
       " (781, 'E', 'E'),\n",
       " (207, 'Y', 'Y'),\n",
       " (28, 'I', 'I'),\n",
       " (203, 'L', 'L'),\n",
       " (43, 'T', 'T'),\n",
       " (183, 'L', 'L'),\n",
       " (86, 'G', 'G'),\n",
       " (536, 'L', 'L'),\n",
       " (92, 'S', 'S'),\n",
       " (419, 'L', 'L'),\n",
       " (180, 'D', 'D'),\n",
       " (158, 'P', 'P'),\n",
       " (714, 'T', 'T'),\n",
       " (44, 'L', 'L'),\n",
       " (219, 'V', 'V'),\n",
       " (171, 'S', 'S'),\n",
       " (34, 'S', 'S'),\n",
       " (143, 'Q', 'Q'),\n",
       " (128, 'C', 'C'),\n",
       " (384, 'R', 'R'),\n",
       " (97, 'R', 'R'),\n",
       " (353, 'N', 'N'),\n",
       " (450, 'F', 'F'),\n",
       " (366, 'I', 'I'),\n",
       " (195, 'W', 'W'),\n",
       " (117, 'R', 'R'),\n",
       " (150, 'M', 'M'),\n",
       " (264, 'C', 'C'),\n",
       " (17, 'S', 'S'),\n",
       " (56, 'G', 'G'),\n",
       " (110, 'N', 'N'),\n",
       " (204, 'R', 'R'),\n",
       " (110, 'D', 'N'),\n",
       " (100, 'S', 'S'),\n",
       " (141, 'F', 'F'),\n",
       " (458, 'A', 'A'),\n",
       " (392, 'A', 'A'),\n",
       " (360, 'K', 'K'),\n",
       " (292, 'D', 'D'),\n",
       " (109, 'P', 'P'),\n",
       " (451, 'R', 'R'),\n",
       " (907, 'F', 'F'),\n",
       " (435, 'L', 'L'),\n",
       " (283, 'P', 'P'),\n",
       " (702, 'E', 'E'),\n",
       " (91, 'L', 'L'),\n",
       " (91, 'S', 'S'),\n",
       " (477, 'T', 'T'),\n",
       " (101, 'V', 'V'),\n",
       " (906, 'D', 'D'),\n",
       " (73, 'T', 'N'),\n",
       " (410, 'T', 'T'),\n",
       " (21, 'A', 'A'),\n",
       " (112, 'K', 'K'),\n",
       " (117, 'H', 'H'),\n",
       " (437, 'A', 'A'),\n",
       " (135, 'V', 'V'),\n",
       " (735, 'T', 'T'),\n",
       " (832, 'L', 'L'),\n",
       " (1195, 'M', 'M'),\n",
       " (34, 'E', 'E'),\n",
       " (130, 'L', 'L'),\n",
       " (467, 'D', 'D'),\n",
       " (381, 'E', 'E'),\n",
       " (126, 'D', 'D'),\n",
       " (104, 'G', 'G'),\n",
       " (21, 'D', 'D'),\n",
       " (1497, 'Y', 'Y'),\n",
       " (207, 'P', 'P'),\n",
       " (82, 'A', 'A'),\n",
       " (481, 'R', 'R'),\n",
       " (713, 'M', 'M'),\n",
       " (21, 'K', 'K'),\n",
       " (676, 'A', 'A'),\n",
       " (1942, 'R', 'R'),\n",
       " (43, 'R', 'R'),\n",
       " (45, 'L', 'L'),\n",
       " (4714, 'G', 'G'),\n",
       " (307, 'L', 'L'),\n",
       " (229, 'D', 'D'),\n",
       " (133, 'N', 'N'),\n",
       " (298, 'P', 'P'),\n",
       " (450, 'E', 'N'),\n",
       " (253, 'T', 'T'),\n",
       " (606, 'E', 'T'),\n",
       " (85, 'S', 'S'),\n",
       " (1719, 'D', 'D'),\n",
       " (77, 'T', 'T'),\n",
       " (789, 'A', 'A'),\n",
       " (468, 'N', 'N'),\n",
       " (87, 'T', 'T'),\n",
       " (530, 'G', 'G'),\n",
       " (2111, 'R', 'R'),\n",
       " (134, 'Y', 'Y'),\n",
       " (585, 'V', 'V'),\n",
       " (796, 'D', 'D'),\n",
       " (523, 'T', 'T')]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9829424307036247"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=0\n",
    "for res in results:\n",
    "    s+=res[1]==res[2]\n",
    "s/len(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
